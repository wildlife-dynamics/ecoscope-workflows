import os
import time

import lithops
import psutil
# from lithops import RetryingFunctionExecutor

# from ecoscope_workflows.tasks.io import get_subjectgroup_observations
from ecoscope_workflows.tasks.setters import set_groupers, set_map_styles
from ecoscope_workflows.tasks.analysis import calculate_time_density

from ecoscope_workflows.tasks.parallelism import split_groups
from ecoscope_workflows.tasks.preprocessing import (
    process_relocations,
    relocations_to_trajectory,
)
from ecoscope_workflows.tasks.results import (
    draw_ecomap,
    draw_ecoplot,
    gather_dashboard,
    merge_widgets,
)
from ecoscope_workflows.tasks.transformation import assign_temporal_column
from ecoscope_workflows.serde import (
    load_gdf_from_hive_partitioned_parquet,
    storage_object_key_from_composite_hivekey,
    persist_html_text,
)
from ecoscope_workflows.testing import generate_synthetic_gps_fixes_dataframe

# represents user input via config file, web frontend form, etc.
# these params would not be hardcoded in a real-world application
params = {
    "set_groupers": dict(
        groupers={"groupers": ["animal_name", "month"]},
    ),
    "set_map_styles": dict(
        map_styles={"map_styles": {}},
    ),
    # synthetic data generator params, these will be different in a real-world application
    "get_subjectgroup_observations": dict(
        # num_fixes=2832,
        # animal_names=[f"{char}o" for char in string.ascii_uppercase],
        num_fixes=100,
        animal_names=["Ao"],
        start_time="2023-01-01",
        time_interval_minutes=30,
        animal_type="Elephant",
        social_structure="solitary",
    ),
    "assign_temporal_column": dict(
        col_name="month",
        time_col="fixtime",
        directive="%B",  # month name
    ),
    "process_relocations": dict(
        filter_point_coords=[[180, 90], [0, 0]],
        relocs_columns=["groupby_col", "fixtime", "junk_status", "geometry"],
    ),
    "relocations_to_trajectory": dict(
        min_length_meters=0.001,
        max_length_meters=10000,
        max_time_secs=3600,
        min_time_secs=1,
        max_speed_kmhr=120,
        min_speed_kmhr=0.0,
    ),
    "calculate_time_density": dict(
        pixel_size=250.0,
        crs="ESRI:102022",
        nodata_value="nan",
        band_count=1,
        max_speed_factor=1.05,
        expansion_factor=1.3,
        percentiles=[50.0, 60.0, 70.0, 80.0, 90.0, 95.0],
    ),
    "draw_ecomap": dict(
        data_type="Polygon",
        style_kws={},
        tile_layer="OpenStreetMap",
        static=False,
        title="Great Map",
        title_kws={},
        scale_kws={},
        north_arrow_kws={},
    ),
    "gather_widget": dict(
        widget_type="ecomap",
        title="Time Density Ecomap",
    ),
}
# override get_subjectgroup_observations with synthetic data generator
# for demonstration purposes; remove this line to use real data
get_subjectgroup_observations = generate_synthetic_gps_fixes_dataframe

if __name__ == "__main__":
    ECOSCOPE_WORKFLOWS_TMP = os.environ.get("ECOSCOPE_WORKFLOWS_TMP", ".")
    ECOSCOPE_WORKFLOWS_RESULTS = os.environ.get("ECOSCOPE_WORKFLOWS_RESULTS", ".")
    JOB_ID = os.environ.get("JOB_ID", f"job-{int(time.monotonic())}")
    TMP_PARQUET = os.path.join(ECOSCOPE_WORKFLOWS_TMP, JOB_ID, "tmp.parquet")
    RESULTS_DIR = os.path.join(ECOSCOPE_WORKFLOWS_RESULTS, JOB_ID, "results")

    # "global" settings
    groupers = set_groupers.replace(validate=True)(**params["set_groupers"])
    map_styles = set_map_styles.replace(validate=True)(**params["set_map_styles"])

    # fetch source table and transform it in preparation for groupby
    df = get_subjectgroup_observations(**params["get_subjectgroup_observations"]).pipe(
        assign_temporal_column.replace(validate=True),
        **params["assign_temporal_column"],
    )

    # generate parallel collection for map-reduce
    time_density_ecomap_groups = split_groups(
        widget_name="time_density_ecomap",
        widget_kws={"title": "Time Density Ecomap"},
        dataframe=df,
        **groupers,
        cache_path=TMP_PARQUET,
    )
    groups = time_density_ecomap_groups  # + second_widget_groups + third_widget_groups

    # define map and reduce functions for map-reduce; TODO: we will need a spec for how to declare
    # these functions in YAML config, so that they can be generated by the compiler

    def log_memory(process: psutil.Process):
        print("Current memory usage: ", process.memory_info().rss / 1e6)

    def create_time_density_ecomap_widget(widget_kws, path, filters) -> dict:
        process = psutil.Process()
        print("Processing", filters, path)
        log_memory(process)
        df = load_gdf_from_hive_partitioned_parquet(
            path=path,
            filters=filters,
            crs="EPSG:4326",  # sketchy
        )
        print("Loaded", df)
        print("Processing relocations")
        log_memory(process)
        reloc = process_relocations.replace(validate=True)(
            df, **params["process_relocations"]
        )
        print("Processing trajectory")
        log_memory(process)
        traj = relocations_to_trajectory.replace(validate=True)(
            reloc, **params["relocations_to_trajectory"]
        )
        td_start = time.time()
        print("Calculating time density")
        log_memory(process)
        td = calculate_time_density.replace(validate=True)(
            traj, **params["calculate_time_density"]
        )
        td_elapsed = time.time() - td_start
        print("Time density calculated in", td_elapsed, "seconds")
        log_memory(process)
        print("Drawing ecomap")
        html_text = draw_ecomap(td, **params["draw_ecomap"])
        log_memory(process)
        print("Persisting ecomap")
        html_path = persist_html_text(
            html_text,
            root_path=os.path.join(
                RESULTS_DIR,
                storage_object_key_from_composite_hivekey(filters),
            ),
        )
        log_memory(process)
        return {
            "widget_type": "ecomap",
            "views": {filters: html_path},
            "title": widget_kws["title"],
        }

    def create_x_ecoplot_widget(widget_kws, path, filters) -> dict:
        df = load_gdf_from_hive_partitioned_parquet(
            path=path,
            filters=filters,
            crs="EPSG:4326",  # sketchy
        )
        plot_html = draw_ecoplot(df, **params["draw_ecoplot"])
        html_path = persist_html_text(
            plot_html,
            root_path=os.path.join(
                RESULTS_DIR,
                storage_object_key_from_composite_hivekey(filters),
            ),
        )
        return {
            "widget_type": "ecoplot",
            "views": {filters: html_path},
            "title": widget_kws["title"],
        }

    def map_function(widget_name, widget_kws, path, filters) -> tuple[tuple, str]:
        if widget_name == "time_density_ecomap":
            return create_time_density_ecomap_widget(widget_kws, path, filters)
        elif widget_name == "x_ecoplot_widget":
            return create_x_ecoplot_widget(widget_kws, path, filters)
        else:
            raise ValueError(f"Unknown widget name '{widget_name}'")

    # Configure the compute backend (local, cloud, etc.) with a configuration file:
    # https://lithops-cloud.github.io/docs/source/configuration.html#configuration-file
    start = time.time()
    fexec = lithops.FunctionExecutor()
    fexec.map(
        map_function=map_function,
        map_iterdata=groups,
    )
    all_widget_views = fexec.get_result(throw_except=False)
    print("Elapsed", round(time.time() - start), "seconds")

    # uncomment for retrying function executor (have not seen the need for this so far)
    # with RetryingFunctionExecutor(fexec) as executor:
    #     futures = executor.map(
    #         map_function,
    #         map_iterdata=groups,
    #         timeout=60,
    #         retries=5,
    #     )
    #     done, pending = executor.wait(futures, throw_except=False)
    #     assert len(pending) == 0
    # outputs = set(f.result() for f in done)
    widgets = merge_widgets(all_widget_views)
    dashboard = gather_dashboard(widgets=widgets, **groupers)
    print(dashboard)
    fexec.plot()
