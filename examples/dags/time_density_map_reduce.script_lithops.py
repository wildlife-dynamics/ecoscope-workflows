import os
import time

import lithops

# from ecoscope_workflows.tasks.io import get_subjectgroup_observations
from ecoscope_workflows.tasks.setters import set_groupers, set_map_styles
from ecoscope_workflows.tasks.analysis import calculate_time_density

from ecoscope_workflows.tasks.parallelism import split_groups
from ecoscope_workflows.tasks.preprocessing import (
    process_relocations,
    relocations_to_trajectory,
)
from ecoscope_workflows.tasks.results import (
    gather_widget,
)
from ecoscope_workflows.tasks.transformation import assign_temporal_column
from ecoscope_workflows.serde import (
    load_gdf_from_hive_partitioned_parquet,
)
from ecoscope_workflows.testing import generate_synthetic_gps_fixes_dataframe

# represents user input via config file, web frontend form, etc.
# these params would not be hardcoded in a real-world application
params = {
    "set_groupers": dict(
        groupers={"groupers": ["animal_name", "month"]},
    ),
    "set_map_styles": dict(
        map_styles={"map_styles": {}},
    ),
    "get_subjectgroup_observations": dict(
        # synthetic data generator params, these
        # will be different in a real-world application
        # num_fixes=2832,
        num_fixes=100,
        start_time="2023-01-01",
        time_interval_minutes=30,
        animal_type="Elephant",
        # animal_names=[f"{char}o" for char in string.ascii_uppercase],  #  # "Bo", "Co", "Do"], #
        animal_names=["Ao"],
        social_structure="solitary",
    ),
    "assign_temporal_column": dict(
        col_name="month",
        time_col="fixtime",
        directive="%B",  # month name
    ),
    "process_relocations": dict(
        filter_point_coords=[[180, 90], [0, 0]],
        relocs_columns=["groupby_col", "fixtime", "junk_status", "geometry"],
    ),
    "relocations_to_trajectory": dict(
        min_length_meters=0.001,
        max_length_meters=10000,
        max_time_secs=3600,
        min_time_secs=1,
        max_speed_kmhr=120,
        min_speed_kmhr=0.0,
    ),
    "calculate_time_density": dict(
        pixel_size=250.0,
        crs="ESRI:102022",
        nodata_value="nan",
        band_count=1,
        max_speed_factor=1.05,
        expansion_factor=1.3,
        percentiles=[50.0, 60.0, 70.0, 80.0, 90.0, 95.0],
    ),
    "draw_ecomap": dict(
        static=False,
        height=1000,
        width=1500,
        search_control=True,
        title="Great Map",
        title_kws={},
        tile_layers=[],
        north_arrow_kws={},
        add_gdf_kws={},
    ),
    "gather_widget": dict(
        widget_type="ecomap",
        title="Time Density Ecomap",
    ),
}
# override get_subjectgroup_observations with synthetic data generator
# for demonstration purposes; remove this line to use real data
get_subjectgroup_observations = generate_synthetic_gps_fixes_dataframe

if __name__ == "__main__":
    ECOSCOPE_WORKFLOWS_TMP = os.environ.get("ECOSCOPE_WORKFLOWS_TMP", ".")
    JOB_ID = os.environ.get("JOB_ID", f"job-{int(time.monotonic())}")
    TMP_PARQUET = os.path.join(ECOSCOPE_WORKFLOWS_TMP, JOB_ID, "tmp.parquet")
    RESULTS_DIR = os.path.join(ECOSCOPE_WORKFLOWS_TMP, JOB_ID, "results")
    # RESULTS_DIR.mkdir(parents=True, exist_ok=True)

    # "global" settings
    groupers = set_groupers.replace(validate=True)(**params["set_groupers"])
    map_styles = set_map_styles.replace(validate=True)(**params["set_map_styles"])

    # fetch source table and transform it in preparation for groupby
    df = get_subjectgroup_observations(**params["get_subjectgroup_observations"]).pipe(
        assign_temporal_column.replace(validate=True),
        **params["assign_temporal_column"],
    )

    # generate parallel collection for map-reduce
    groups = split_groups(df, **groupers, cache_path=TMP_PARQUET)

    # groups = groups * 12 # for testing
    # define map and reduce functions for map-reduce; TODO: we will need a spec for how to declare
    # these functions in YAML config, so that they can be generated by the compiler
    def map_function(path, filters) -> tuple[tuple, str]:
        print("Processing", filters, path)
        df = load_gdf_from_hive_partitioned_parquet(
            path=path,
            filters=filters,
            crs="EPSG:4326",  # sketchy
        )
        print("Loaded", df)
        print("Processing relocations")
        reloc = process_relocations.replace(validate=True)(
            df, **params["process_relocations"]
        )
        print("Processing trajectory")
        traj = relocations_to_trajectory.replace(validate=True)(
            reloc, **params["relocations_to_trajectory"]
        )
        td_start = time.time()
        print("Calculating time density")
        td = calculate_time_density.replace(validate=True)(
            traj, **params["calculate_time_density"]
        )
        td_elapsed = time.time() - td_start
        print("Time density calculated in", td_elapsed, "seconds")
        print("Drawing ecomap")
        # html_text = draw_ecomap(td, **params["draw_ecomap"])
        # print("Persisting ecomap")
        # result = filters, persist_html_text(
        #     html_text,
        #     root_path=os.path.join(
        #         RESULTS_DIR,
        #         storage_object_key_from_composite_hivekey(filters),
        #     ),
        # )
        result = len(td)
        return result

    def reduce_function(results: list[tuple[tuple, str]]) -> list[str]:
        print("Running reducer function")
        return gather_widget.replace(validate=True)(results, **params["gather_widget"])

    # this can parallelize on local threads, gcp cloud run, or other cloud serverless
    # compute backends, depending on the lithops configuration set at runtime.
    # time_density_ecomap_widget = map_reduce(
    #     groups=groups,
    #     map_function=map_function,
    #     reduce_function=reduce_function,
    # )
    # from lithops.retries import RetryingFunctionExecutor

    # Configure the compute backend (local, cloud, etc.) with a configuration file:
    # https://lithops-cloud.github.io/docs/source/configuration.html#configuration-file
    start = time.time()
    fexec = lithops.FunctionExecutor()
    fexec.map(
        map_function=map_function,
        map_iterdata=groups,
    )
    outputs = fexec.get_result()
    # done, _ = fexec.wait(download_results=False)
    # outputs = map_function(**groups[0])
    elapsed = time.time() - start
    # dashboard = gather_dashboard(widgets=[time_density_ecomap_widget], **groupers)
    print(outputs)
    print("Elapsed", round(elapsed), "seconds")
    fexec.plot()
