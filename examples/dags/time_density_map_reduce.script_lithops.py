import os
import time

# from ecoscope_workflows.tasks.io import get_subjectgroup_observations
from ecoscope_workflows.tasks.setters import set_groupers, set_map_styles
from ecoscope_workflows.tasks.analysis import calculate_time_density

from ecoscope_workflows.tasks.parallelism import split_groups, map_reduce
from ecoscope_workflows.tasks.preprocessing import (
    process_relocations,
    relocations_to_trajectory,
)
from ecoscope_workflows.tasks.results import (
    draw_ecomap,
    gather_dashboard,
    gather_widget,
)
from ecoscope_workflows.tasks.transformation import assign_temporal_column
from ecoscope_workflows.serde import (
    load_gdf_from_hive_partitioned_parquet,
    persist_html_text,
    storage_object_key_from_composite_hivekey,
)
from ecoscope_workflows.testing import generate_synthetic_gps_fixes_dataframe

# represents user input via config file, web frontend form, etc.
# these params would not be hardcoded in a real-world application
params = {
    "set_groupers": dict(
        groupers={"groupers": ["animal_name", "month"]},
    ),
    "set_map_styles": dict(
        map_styles={"map_styles": {}},
    ),
    "get_subjectgroup_observations": dict(
        # synthetic data generator params, these
        # will be different in a real-world application
        num_fixes=2000,
        start_time="2023-01-01",
        time_interval_minutes=30,
        animal_type="Elephant",
        animal_names=["Bo", "Mo", "Jo", "Lo", "Po", "Ro"],
        social_structure="solitary",
    ),
    "assign_temporal_column": dict(
        col_name="month",
        time_col="fixtime",
        directive="%B",  # month name
    ),
    "process_relocations": dict(
        filter_point_coords=[[180, 90], [0, 0]],
        relocs_columns=["groupby_col", "fixtime", "junk_status", "geometry"],
    ),
    "relocations_to_trajectory": dict(
        min_length_meters=0.001,
        max_length_meters=10000,
        max_time_secs=3600,
        min_time_secs=1,
        max_speed_kmhr=120,
        min_speed_kmhr=0.0,
    ),
    "calculate_time_density": dict(
        pixel_size=250.0,
        crs="ESRI:102022",
        nodata_value="nan",
        band_count=1,
        max_speed_factor=1.05,
        expansion_factor=1.3,
        percentiles=[50.0, 60.0, 70.0, 80.0, 90.0, 95.0],
    ),
    "draw_ecomap": dict(
        static=False,
        height=1000,
        width=1500,
        search_control=True,
        title="Great Map",
        title_kws={},
        tile_layers=[],
        north_arrow_kws={},
        add_gdf_kws={},
    ),
    "gather_widget": dict(
        widget_type="ecomap",
        title="Time Density Ecomap",
    ),
}
# override get_subjectgroup_observations with synthetic data generator
# for demonstration purposes; remove this line to use real data
get_subjectgroup_observations = generate_synthetic_gps_fixes_dataframe

if __name__ == "__main__":
    ECOSCOPE_WORKFLOWS_TMP = os.environ.get("ECOSCOPE_WORKFLOWS_TMP", ".")
    JOB_ID = os.environ.get("JOB_ID", f"job-{int(time.monotonic())}")
    TMP_PARQUET = os.path.join(ECOSCOPE_WORKFLOWS_TMP, JOB_ID, "tmp.parquet")
    RESULTS_DIR = os.path.join(ECOSCOPE_WORKFLOWS_TMP, JOB_ID, "results")
    # RESULTS_DIR.mkdir(parents=True, exist_ok=True)

    # "global" settings
    groupers = set_groupers.replace(validate=True)(**params["set_groupers"])
    map_styles = set_map_styles.replace(validate=True)(**params["set_map_styles"])

    # fetch source table and transform it in preparation for groupby
    df = get_subjectgroup_observations(**params["get_subjectgroup_observations"]).pipe(
        assign_temporal_column.replace(validate=True),
        **params["assign_temporal_column"],
    )

    # generate parallel collection for map-reduce
    groups = split_groups(df, **groupers, cache_path=TMP_PARQUET)

    # define map and reduce functions for map-reduce; TODO: we will need a spec for how to declare
    # these functions in YAML config, so that they can be generated by the compiler
    def map_function(element: tuple[tuple, str]) -> tuple[tuple, str]:
        composite_hivekey, df_url = element
        print("Processing", composite_hivekey, df_url)
        df = load_gdf_from_hive_partitioned_parquet(
            path=df_url,
            filters=composite_hivekey,
            crs="EPSG:4326",  # sketchy
        )
        print("Loaded", df)
        print("Processing relocations")
        reloc = process_relocations.replace(validate=True)(
            df, **params["process_relocations"]
        )
        print("Processing trajectory")
        traj = relocations_to_trajectory.replace(validate=True)(
            reloc, **params["relocations_to_trajectory"]
        )
        print("Calculating time density")
        td = calculate_time_density.replace(validate=True)(
            traj, **params["calculate_time_density"]
        )
        print("Drawing ecomap")
        html_text = draw_ecomap.replace(validate=True)(td, **params["draw_ecomap"])
        print("Persisting ecomap")
        return composite_hivekey, persist_html_text(
            html_text,
            root_path=os.path.join(
                RESULTS_DIR,
                storage_object_key_from_composite_hivekey(composite_hivekey),
            ),
        )

    def reduce_function(results: list[tuple[tuple, str]]) -> list[str]:
        return gather_widget.replace(validate=True)(results, **params["gather_widget"])

    # this can parallelize on local threads, gcp cloud run, or other cloud serverless
    # compute backends, depending on the lithops configuration set at runtime.
    time_density_ecomap_widget = map_reduce(
        groups=groups,
        map_function=map_function,
        reduce_function=reduce_function,
    )

    dashboard = gather_dashboard(widgets=[time_density_ecomap_widget], **groupers)
    print(dashboard)
